# WLAN-Analyse-Tool Test Suite

Diese Test-Suite bietet umfassende Tests fÃ¼r das WLAN-Analyse-Tool mit verschiedenen Test-Kategorien und Performance-Messungen.

## ğŸ“ Test-Struktur

```
pytest/
â”œâ”€â”€ conftest.py              # Gemeinsame Fixtures und Konfiguration
â”œâ”€â”€ test_storage.py          # Tests fÃ¼r Storage-Module (state, database, data_models)
â”œâ”€â”€ test_analysis.py         # Tests fÃ¼r Analysis-Module (logic, device_profiler)
â”œâ”€â”€ test_capture.py          # Tests fÃ¼r Capture-Module (sniffer)
â”œâ”€â”€ test_presentation.py     # Tests fÃ¼r Presentation-Module (cli, tui, reporting)
â”œâ”€â”€ test_utils.py            # Tests fÃ¼r Utils-Module
â”œâ”€â”€ test_controllers.py      # Tests fÃ¼r Controller-Module
â”œâ”€â”€ test_integration.py      # Integrationstests
â”œâ”€â”€ test_performance.py      # Performance-Tests
â”œâ”€â”€ pytest.ini              # Pytest-Konfiguration
â”œâ”€â”€ requirements-test.txt    # Test-AbhÃ¤ngigkeiten
â”œâ”€â”€ Makefile                 # Test-AusfÃ¼hrungs-Ziele
â””â”€â”€ README.md               # Diese Datei
```

## ğŸš€ Schnellstart

### Installation der Test-AbhÃ¤ngigkeiten

```bash
pip install -r requirements-test.txt
```

### Alle Tests ausfÃ¼hren

```bash
make test
# oder
pytest -v
```

### Tests mit Coverage-Report

```bash
make test-coverage
# oder
pytest --cov=wlan_tool --cov-report=html:htmlcov
```

## ğŸ“Š Test-Kategorien

### 1. Unit-Tests
Testen einzelne Funktionen und Klassen isoliert:

```bash
make test-unit
# oder
pytest -m "not integration and not performance"
```

**Abgedeckte Module:**
- `test_storage.py` - State-Management, Datenbank, Datenmodelle
- `test_analysis.py` - Analyse-Logik, Clustering, Inferenz
- `test_capture.py` - Paket-Parsing, Channel-Hopping
- `test_presentation.py` - CLI, TUI, Reporting
- `test_utils.py` - Utility-Funktionen, OUI-Lookup
- `test_controllers.py` - Controller-Logik

### 2. Integrationstests
Testen das Zusammenspiel verschiedener Module:

```bash
make test-integration
# oder
pytest -m "integration"
```

**Abgedeckte Bereiche:**
- End-to-End-Workflows
- Datenfluss zwischen Modulen
- Datenbank-Integration
- Konfigurations-Integration
- Fehlerbehandlung und Recovery

### 3. Performance-Tests
Messen und validieren Performance-Charakteristika:

```bash
make test-performance
# oder
pytest -m "performance"
```

**Gemessene Metriken:**
- Paket-Verarbeitungsgeschwindigkeit
- Analyse-Performance
- Speicherverbrauch
- Datenbank-Performance
- Skalierbarkeit

## ğŸ·ï¸ Test-Marker

Die Tests sind mit verschiedenen Markern kategorisiert:

- `@pytest.mark.slow` - Langsame Tests (> 1 Sekunde)
- `@pytest.mark.integration` - Integrationstests
- `@pytest.mark.unit` - Unit-Tests
- `@pytest.mark.performance` - Performance-Tests
- `@pytest.mark.network` - Tests mit Netzwerk-Zugriff
- `@pytest.mark.hardware` - Tests mit Hardware-Anforderungen

### Tests nach Marker ausfÃ¼hren

```bash
# Nur schnelle Tests
make test-fast
pytest -m "not slow"

# Nur langsame Tests
make test-slow
pytest -m "slow"

# Tests mit Netzwerk-Zugriff
make test-network
pytest -m "network"
```

## ğŸ“ˆ Coverage-Report

### HTML-Report generieren

```bash
make test-coverage
```

Der HTML-Report wird in `htmlcov/index.html` generiert.

### Coverage-Schwellenwerte

- **Minimum Coverage**: 80%
- **Aktuelle Konfiguration**: Siehe `pytest.ini`

### Coverage-Report anzeigen

```bash
# Terminal-Report
pytest --cov=wlan_tool --cov-report=term-missing

# XML-Report (fÃ¼r CI/CD)
pytest --cov=wlan_tool --cov-report=xml
```

## ğŸ”§ Test-Konfiguration

### pytest.ini

Die Test-Konfiguration ist in `pytest.ini` definiert:

- **Python-Pfad**: Automatisch auf `..` gesetzt
- **Test-Pfade**: Aktuelles Verzeichnis
- **Coverage**: 80% Mindest-Coverage
- **Warnings**: Gefiltert fÃ¼r bessere Lesbarkeit
- **Output**: Farbig und detailliert

### Anpassung der Konfiguration

```ini
# pytest.ini anpassen
[tool:pytest]
addopts = -v --tb=short --cov=wlan_tool --cov-fail-under=80
```

## ğŸ› Debugging

### Tests im Debug-Modus ausfÃ¼hren

```bash
make test-debug
# oder
pytest -vv --tb=long --pdb
```

### Einzelne Tests ausfÃ¼hren

```bash
# Spezifische Test-Datei
pytest test_storage.py -v

# Spezifische Test-Klasse
pytest test_storage.py::TestWifiAnalysisState -v

# Spezifische Test-Funktion
pytest test_storage.py::TestWifiAnalysisState::test_state_initialization -v
```

### Verbose-Ausgabe

```bash
make test-verbose
# oder
pytest -vv --tb=long
```

## ğŸ“Š Performance-Benchmarks

### Benchmark-Ergebnisse anzeigen

```bash
pytest test_performance.py --benchmark-only
```

### Performance-Tests mit Profiling

```bash
# Memory-Profiling
pytest test_performance.py::TestMemoryPerformance -v

# CPU-Profiling
pytest test_performance.py::TestAnalysisPerformance -v --profile
```

## ğŸ§¹ Cleanup

### Test-Artefakte bereinigen

```bash
make clean
```

Entfernt:
- Coverage-Reports (`htmlcov/`, `.coverage`)
- Cache-Dateien (`.pytest_cache/`)
- TemporÃ¤re Datenbanken (`*.db`)
- Test-Ausgabedateien (`/tmp/test_*`)

## ğŸ”„ CI/CD-Integration

### GitHub Actions Beispiel

```yaml
name: Tests
on: [push, pull_request]
jobs:
  test:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v3
    - name: Set up Python
      uses: actions/setup-python@v3
      with:
        python-version: '3.9'
    - name: Install dependencies
      run: |
        pip install -r requirements.txt
        pip install -r pytest/requirements-test.txt
    - name: Run tests
      run: |
        cd pytest
        make test-coverage
    - name: Upload coverage
      uses: codecov/codecov-action@v3
```

### Lokale CI-Simulation

```bash
# Alle Tests mit Coverage
make test-coverage

# Nur kritische Tests
make test-unit

# Performance-Tests
make test-performance
```

## ğŸ“ Test-Dokumentation

### Fixtures

Alle gemeinsamen Fixtures sind in `conftest.py` definiert:

- `sample_events` - Test-WiFi-Events
- `populated_state` - WifiAnalysisState mit Testdaten
- `in_memory_db` - In-Memory SQLite-Datenbank
- `mock_console` - Mock Console-Objekt
- `sample_scapy_packet` - Test-Scapy-Paket

### Test-Daten

Test-Daten werden dynamisch generiert und sind in den Fixtures definiert. FÃ¼r reproduzierbare Tests werden feste Zeitstempel verwendet.

### Mocking

Umfangreiches Mocking fÃ¼r:
- Externe AbhÃ¤ngigkeiten (scapy, sklearn, etc.)
- System-Aufrufe (subprocess, file I/O)
- Netzwerk-Zugriffe (OUI-Download)
- Hardware-Zugriffe (WLAN-Interfaces)

## ğŸš¨ Bekannte EinschrÃ¤nkungen

1. **Hardware-Tests**: BenÃ¶tigen echte WLAN-Hardware
2. **Netzwerk-Tests**: BenÃ¶tigen Internet-Verbindung
3. **Performance-Tests**: AbhÃ¤ngig von System-Ressourcen
4. **Integrationstests**: KÃ¶nnen bei fehlenden AbhÃ¤ngigkeiten fehlschlagen

## ğŸ¤ Beitragen

### Neue Tests hinzufÃ¼gen

1. WÃ¤hle die passende Test-Datei oder erstelle eine neue
2. Verwende bestehende Fixtures aus `conftest.py`
3. FÃ¼ge passende Marker hinzu (`@pytest.mark.unit`, etc.)
4. Dokumentiere den Test mit Docstrings

### Test-Standards

- **Naming**: `test_<function_name>_<scenario>`
- **Structure**: Arrange, Act, Assert
- **Coverage**: Mindestens 80% Code-Coverage
- **Performance**: Unit-Tests < 1s, Integration-Tests < 10s

### Test-Review-Checkliste

- [ ] Test ist deterministisch (keine Race Conditions)
- [ ] Test ist isoliert (keine AbhÃ¤ngigkeiten zu anderen Tests)
- [ ] Test ist schnell (< 1s fÃ¼r Unit-Tests)
- [ ] Test hat aussagekrÃ¤ftige Assertions
- [ ] Test ist gut dokumentiert
- [ ] Test verwendet passende Fixtures